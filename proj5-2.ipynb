{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Karuna Gujar\n",
    "CSCI 6350-001 \n",
    "Project #5\n",
    "Due: 03/23/20\n",
    "\n",
    "Description: takes as input a file containing nouns or verbs having multiple (WordNet) senses, \n",
    "and determine which of two senses are used in accompanying sentences.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.wsd import lesk\n",
    "from time import time\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.data import find\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from statistics import mean \n",
    "from operator import is_not\n",
    "from functools import partial\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet_ic as wic  # import that allows loading of information content\n",
    "ic = wic.ic('ic-brown.dat')  \n",
    "\n",
    "\n",
    "#Read the input file and extract the word, POS, sesnses and the sentences \n",
    "#and save it in a dictionary\n",
    "fileName = 'sentences.txt'\n",
    "sentenceSense = {}\n",
    "sentences = []\n",
    "f = open(fileName, 'r', encoding='utf8')\n",
    "lines = f.readlines()\n",
    "i_d=0\n",
    "i = 0;\n",
    "for line in lines:\n",
    "    word = ''\n",
    "    pos = ''\n",
    "    sense1 = ''\n",
    "    sense2 = ''\n",
    "    if line != '':\n",
    "        words = line.split()\n",
    "        firstWord = words[0]\n",
    "        if not firstWord[0].isupper():    \n",
    "            i_d = i_d + 1            \n",
    "            sentences = []\n",
    "            word = words[0]\n",
    "            pos = words[1]\n",
    "            sense1 = words[2]\n",
    "            sense2 = words [3]\n",
    "            sentenceSense[i_d] = {'word': word, 'pos': pos, 'sense1': sense1, 'sense2':sense2, \n",
    "                            'sentences': sentences}\n",
    "            if(i == len(lines)-1):\n",
    "                sentenceSense[i_d] = {'word': word, 'pos': pos, 'sense1': sense1, 'sense2':sense2, \n",
    "                                'sentences': sentences}\n",
    "        else:\n",
    "            sentences = sentenceSense[i_d]['sentences']\n",
    "            sentences.append(line)\n",
    "            sentenceSense[i_d]['sentences'] = sentences             \n",
    "        \n",
    "    i = i + 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most bats eat mainly insects and bugs, but others also feed on fruits.      bat.n.01\n",
      "The ball flew off the bat with a loud crack, and everyone knew it was a home run.      bat.n.02\n",
      "The sound emitted by the bat has an extremely high pitch, beyong the range of human hearing.      bat.n.01\n",
      "One of the weapons used in the crime was a bat.      bat.n.02\n",
      "Grandma, smoking her pipe, turned around and handed the child a glass of water.      pipe.n.02\n",
      "Some day he was going to pipe water into the barn making it easier to smoke jerky.      pipe.n.02\n",
      "The frozen pipes burst overnight, causing a leak that badly damaged the kitchen.      pipe.n.02\n",
      "As he looked at the water, Jonas filled his pipe and lit it, blowing short puffs into the air.      pipe.n.02\n",
      "Next year she would graduate and would be moving away to college, far from the place she loved.      move.v.04\n",
      "Why would a wealthy man move to the country and become a veterinarian?      move.v.04\n",
      "Gina's eulogy was simple but moving, and left everyone with a deep longing for the old woman.      move.v.04\n",
      "In explaining the bleak circumstances the puppies faced, he hoped he'd move the family to adopt them.      move.v.04\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#Get tokens of the given sentence\n",
    "def get_tokens( sentence):\n",
    "    return [lemmatizer.lemmatize(x) for x in [y for y in word_tokenize(sentence) if y not in stopWords]]\n",
    "\n",
    "#Get unigrams, bigrams and trigrams for a given snetence\n",
    "def get_unibitri( sentence):\n",
    "    s_ngrams = []\n",
    "\n",
    "    token = get_tokens( sentence)\n",
    "    s_ngrams.extend(list(ngrams(token, 1)))\n",
    "    s_ngrams.extend(list(ngrams(token, 2)))\n",
    "    s_ngrams.extend(list(ngrams(token, 3)))\n",
    "    return s_ngrams\n",
    "\n",
    "#Get the uni, bi, tri grams of the definitions of hyponyms for the given sense\n",
    "'''\n",
    "def get_hyponym_defs( cur_synset):\n",
    "    s_ngrams = []\n",
    "    for item in wn.synset(cur_synset).hyponyms():\n",
    "        s_ngrams.extend(get_unibitri(item.definition()))\n",
    "    return s_ngrams\n",
    "\n",
    "#Get the uni, bi, tri grams of the definitions of hypernyms for the given sense\n",
    "#Note: I am not using this method in my current approach but kept it for documentation purpose\n",
    "def get_hypernym_defs( cur_synset):\n",
    "    s_ngrams = []\n",
    "    for item in wn.synset(cur_synset).hypernyms():\n",
    "        s_ngrams.extend(get_unibitri(item.definition()))\n",
    "    return s_ngrams\n",
    "\n",
    "#Matches sentence and definition ignoring supplied words(for instance stopwaords)\n",
    "#Note: I am not using this method in my current approach but kept it for documentation purpose\n",
    "def matching_ngrams( source, target,ignore=[]):\n",
    "    matches = []\n",
    "    for item in source:\n",
    "        for item1 in target:\n",
    "            if item==item1:\n",
    "                matches.append(item)\n",
    "    ignore_tuples = [tuple([x]) for x in ignore]\n",
    "    matches = [x for x in matches if x not in ignore_tuples]\n",
    "            \n",
    "    return matches\n",
    "'''\n",
    "\n",
    "#Gets the sense for every word in the sentence and calculates the similarity \n",
    "#score with the target sense. Then, the max score is returned. \n",
    "#the method can take an argument to provide the flexibiltiy of returning the average, max average or average max.\n",
    "def get_similarity(target, sentence, score=\"max\"):\n",
    "\n",
    "    s_tokens = get_tokens( sentence)\n",
    "    t_tok = get_tokens( target.split(\".\")[0])\n",
    "    s_x=[]\n",
    "    for token in s_tokens:\n",
    "        if stemmer.stem(token)==stemmer.stem(t_tok[0]):\n",
    "            continue\n",
    "        s_x.append(list(filter(None.__ne__,[wn.synset(target).path_similarity(y) for y in wn.synsets(token)])))\n",
    "    if score ==\"max\":\n",
    "        return max([max(x) for x in s_x if len(x) > 0])\n",
    "    if score ==\"avg\":\n",
    "        return mean([mean(x) for x in s_x if len(x) > 0])\n",
    "    if score ==\"max_avg\":\n",
    "        return mean([max(x) for x in s_x if len(x) > 0])\n",
    "    if score ==\"avg_max\":\n",
    "        return max([mean(x) for x in s_x if len(x) > 0])\n",
    "\n",
    "            \n",
    "\n",
    "sentence_sense=[]            \n",
    "for item in sentenceSense.items():\n",
    "    '''sense1_hypo = get_hyponym_defs(item[1][\"sense1\"])\n",
    "    sense2_hypo = get_hyponym_defs(item[1][\"sense2\"])\n",
    "    \n",
    "    sense1_hyper = get_hypernym_defs(item[1][\"sense1\"])\n",
    "    sense2_hyper = get_hypernym_defs(item[1][\"sense2\"])\n",
    "    \n",
    "    match_1=0\n",
    "    match_2=0'''\n",
    "\n",
    "    score = \"avg_max\"\n",
    "    for sentence in item[1][\"sentences\"]:\n",
    "        #ignore_list=[]\n",
    "        #ignore_list.append(item[1][\"word\"])\n",
    "\n",
    "        #print(sentence)\n",
    "        #s_ngrams=get_unibitri(sentence)\n",
    "        #print(matching_ngrams(s_ngrams,sense1_hypo,ignore_list))\n",
    "        \n",
    "        a, b = get_similarity(item[1][\"sense1\"],sentence,score), get_similarity(item[1][\"sense2\"],sentence,score)\n",
    "        \n",
    "        #if the score for both the provided senses is same, NLTK lesk algorithm is used.\n",
    "        if a==b:\n",
    "            cur_answer = lesk(sentence, item[1][\"word\"] ,synsets=[wn.synset(item[1][\"sense1\"]),wn.synset(item[1][\"sense2\"])])\n",
    "            sentence_sense.append((sentence, cur_answer.name()))\n",
    "        else:\n",
    "        \n",
    "            if a > b: sentence_sense.append((sentence,item[1][\"sense1\"]))\n",
    "            else: sentence_sense.append((sentence,item[1][\"sense2\"]))\n",
    "\n",
    "#print(sentence_sense)\n",
    "for s in sentence_sense:\n",
    "    print(s[0].strip(),'    ', s[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed explanation of the scoring method you chose:\n",
    "o How did you arrive at your final scoring method?\n",
    "    I tried different approaches. I started with making a word gloss with the synonyms, hypernyms, hyponyms, meronyms, holonyms of the given word, the tokens of the definition and example for the given sense. Then used this gloss to get an overlap score with the given sentence. But this approach did not work. Every time I was getting the first sense in the list as the output.\n",
    "    Another approach: I used hypernym and hyponym definitions of the target word sense to compute similarity with the given sentences. The similarity was computed using matching unigrams, bigrams and trigrams. Unfortunately I could only find matches for “bat” first example.\n",
    "    Next approach I tried worked better with almost 70% (8 out of 12) accuracy by using path similarity. I am finding the path similarity score between the sense for every stemmed token in the sentence and the target sense and getting the maximum of those scores (for each sentence). Performing this with both the given senses as target. Later whichever has maximum score is the answer. If the score comes to be the same for both targets, I am using NLTK Lesk to get to the answer.\n",
    "\n",
    "\n",
    "o Why did you choose the features you did?\n",
    " I did a lot of experimentation and trial and error approach to get to these features. \n",
    "In every approach I experimented, the goal was extracting all the senses for all the words in the given sentence and find ones related to the target word sense.\n",
    "\n",
    "o Why do you think the similarity measures you chose were the best for the task?\n",
    "I am using path_similarty from NLTK. This method returns a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. \n",
    "\n",
    "\n",
    "o Were WordNet features or vector embedding features more important to your method? Why? \n",
    "WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. However it interlinks not just word forms—strings of letters—but specific senses of words. Hence WordNet features are more important.\n",
    "\n",
    "An analysis of expected performance against unseen sentences\n",
    "o What input may cause it to excel, and when it may not perform well? o Address this for both nouns and verbs.\n",
    "The approach won’t work if the similarity score is same. If the similarity score is same, I am using NLTK Lesk to find the answer. But Lesk performance is not guaranteed. The similarity is scores for senses will be same if the sentence contains word that could be associated with any of the given senses. For example: As he looked at the water, Jonas filled his pipe and lit it, blowing short puffs into the air. The words “water” and “filled” lead to exactly same scores for both pipe.n.01 and pipe.n.02 senses.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
